---
title: "Final Candy"
output:
  pdf_document: default
  html_document: default
date: '2022-12-06'
---

```{r}

library(readr)
IPG3113N <- read_csv("/Users/danieltamayo/Desktop/CandyFinal.csv")

candy_ts <- ts(IPG3113N$IPG3113N,frequency = 12,start=c(2012,10))
Window_candy_TS <-window(candy_ts, start = 2012, end = 2022)
## Plot and Inference
plot(Window_candy_TS)


```

## From this plot, we can see that over time from 2012 to 2022 there appears to be a seasonal component of candy production in the US. There appears to be an element of seasonality as well. This element of seasonality appears to happen once a year. 



```{r}

#Central Tendency
summary(Window_candy_TS)

```

## Including Plots

You can also embed plots, for example:

```{r}
boxplot(Window_candy_TS)
```

## FRom this we can see that the Data seems to be pretty unifromly distributed; The mean and the median are about the same. However most of the data lies between the min and the median/mean. 


```{r}
#Decomposition
stl_decomp <- stl(Window_candy_TS,s.window ="periodic")
plot(stl_decomp)


```


## From the decomposition, we can see that there is quite a strong element of seasonality in the Candy Time Series. 

## ## Since the variation of seasonality(widths and heights) does not change over time, we can see that this decomposition is in fact additive.
```{r}
#Decomposition
Window_candy_TS

```

## Here are the values of the seasonal monthly indices. 


```{r}
#Decomposition

tapply(Window_candy_TS,cycle(Window_candy_TS),mean)

```

## These are the average values for the months from our Time Series. From this, we can see that based on the average, The most Candy is produced in December and the least amount of Candy is produced in May. 

##I believe the most Candy is produced in December for Holiday season and in May there is not that much because there are not that many known festivities in May that involve candy. 

ï‚· Show the plot for time series adjusted for seasonality. Overlay this with the line for 
actual time series? Does seasonality have big fluctuations to the value of time series? 


```{r}
#Decomposition
plot(Window_candy_TS)
lines(seasadj(stl_decomp), col="blue")
```
## This is the graph of our regular TS and our seasonaly adjusted Time Series in Blue over that. 

```{r}
#Naive Method
naive_forecast <- naive(Window_candy_TS,20)
plot(naive_forecast)
```
```{r}
#Naive Method

plot(naive_forecast$residuals)
```

##Since there appears to be seasonlaity and a trend in the resiudals. This shows that the naive forecast is not a good fit for this data set.


```{r}
#Naive Method

hist(naive_forecast$residuals)
```
##This shows that he data is pretty normally distributed. 

```{r}
#Naive Method

Acf(naive_forecast$residuals)
```
##This indicates that there seems to be a seasonal component, there are also some values in the residuals which are statistically significant. 

```{r}
#Naive Method

accuracy(naive_forecast)
```
```{r}
#Naive Method


naive_forecast_12 <- naive(Window_candy_TS,12)
naive_forecast_12
```
```{r}
#Naive Method



plot(naive_forecast_12)
```
##Overall, the Naive forecast is not the best since it does not take into account seasonality and teh trend. It says the value will be 113. 

```{r}
#Simple Moving Averages

MA3_forecast <- ma(candy_ts,order=3)
MA6_forecast <- ma(candy_ts,order=6)
MA9_forecast <- ma(candy_ts,order=9)

plot(Window_candy_TS)
lines(MA3_forecast,col="Red")
lines(MA6_forecast,col="Blue")
lines(MA9_forecast,col="Green")


```
## As the moving average goes up we can see that the seasonality is less wide. Overall, this smooths the graph out. 








```{r}
#Simple Smoothing
SES_candy <- ses(Window_candy_TS)
SES_candy_forecast <- forecast(SES_candy)
plot(SES_candy_forecast)
```
```{r}
#Simple Smoothing

SES_candy
```


```{r}
#Simple Smoothing

SES_candy$alpha
```

```{r}
#Simple Smoothing

SES_candy$beta
```


```{r}
#Simple Smoothing

SES_candy$gamma
```

```{r}
#Simple Smoothing

plot(SES_candy$residuals)
```


```{r}
#Simple Smoothing

hist(SES_candy$residuals)
```

```{r}
#Simple Smoothing

Acf(SES_candy$residuals)
```
##Overall not good- seasonality and significant values







```{r}
#Holt Winters
HW_candy <- HoltWinters(Window_candy_TS)
HW_candy_forecast <- forecast(HW_candy,h=12)
plot(HW_candy_forecast)
```


```{r}
#Holt Winters
HW_candy
```

## From this we can see that value of alpha is 0.75, the value of beta is 0 and the value of gamma is 0.488. 
##Alpha signifies the level of the smoothing - since it is .75, later values have more weight. ##Beta signifies the level for trend smoothing - since it is 0, earlier values have all teh weight. 
##Gamma refers to the seasonal component of the timeseries sinec it is .48 this means that it is right in the middle of earlier and later values. 

```{r}
#Holt Winters
HW_candy$Sigma

```
The value of sigma is null

```{r}
#ARIMA
plot(HW_candy_forecast$residuals)

```

```{r}
#ARIMA
hist(HW_candy_forecast$residuals)


```

## This shows there is a normal distribution in residuals

```{r}
#HW
Acf(HW_candy_forecast$residuals)

```
## THis looks good, no trends, seasonality, all points are not significant

```{r}
#HW

accuracy(HW_candy_forecast)
```

```{r}
#HW

plot(forecast(HW_candy_forecast, h=12))
```

```{r}
#HW

forecast(HW_candy_forecast, h=12)
```

##Overall, we can see that this a very good model. The model in a year is predicted to be 114.0443. We can tell this is a good model by the residuals. 

```{r}
#ARIMA
plot(Window_candy_TS)

```
```{r}
#ARIMA
adf.test(Window_candy_TS)
kpss.test(Window_candy_TS)
```


##  The TS is stationary since when we run the KPSS test, the p-value is less than .05. 
```{r}
#ARIMA

nsdiffs(Window_candy_TS)
```
## From this we can see that the number of differences in order to make the data stationary is 1. 

## The seasonality component is needed since out time series has a seasonality component. 

```{r}
#ARIMA

Candy_diff <- diff(Window_candy_TS, differences=1)
plot(Candy_diff)


```
##This is our plot including the difference of 1. 


```{r}
#ARIMA


tsdisplay(UKgasdiff1)

```
```{r}
#ARIMA


fit1 <- auto.arima(candy_ts)
fit1
```

## From above, we can see that sigma^2 = 14.95, AIC = 612.72 and BIC = 626.18

```{r}
#ARIMA


fit3 <- auto.arima(Window_candy_TS,trace=TRUE, stepwise = FALSE )
```

## Based on the these auto arima functions run, we can see that the best model is ARIMA(1,0,0)(2,1,0)[12] with drift 


```{r}
#ARIMA


plot(fit1$residuals)
```
## These residuals looks very good since it appears to be white noise - There appears to be no seasonality or trend. 


```{r}
#ARIMA


hist(fit1$residuals)
```
## This histogram plot seems to indicate that the data is normally distributed which is good for our residuals. 


```{r}
#ARIMA


Acf(fit1$residuals)
```
## This plot indicates that this plot is indeed the best because there is no trend or seasonality in the data. There values are also not statistically significant except for one and that appears to be a mistake. 

```{r}
#ARIMA

accuracy(fit1)
```

```{r}
#ARIMA


oneyear_arima <- (forecast(fit1, h=12))
plot(oneyear_arima)
```


```{r}
#ARIMA


oneyear_arima
```


```{r}
#ARIMA


twoyear_arima <- (forecast(fit1, h=24))
plot(twoyear_arima)
```

```{r}
#ARIMA


twoyear_arima
```
## We will see that in October 2023(a year from this point in time) the prediction is giving us 130 and in two years time(October 2024) we are getting 133. 


## Overall we can see that this forecatsing technique is very good; the residuals are random and not significant and we can see the forecast is incorporating the positive trend as well as seasonality. 

